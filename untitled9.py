# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y6cClLwY2Z7S3dgQq72sug-M0_F3L35O
"""

# Importing required libraries

# Dense layers for fully connected networks
# Droout to reduce overfitting
# Flatten - multi dimension to one dimension conversion
# Batchnormalization to normalise the inputs by re-centering and re-scaling
# Creates activation with same distribution
# Maxnormalisation is also used to prevent overfitting

import tensorflow as tf
from tensorflow.keras import layers, models
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization
from keras.layers.convolutional import MaxPooling2D, Conv2D
from keras.utils import np_utils
from keras.constraints import maxnorm

# Importing dataset

from keras.datasets import cifar10

# Loading the dataset

(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# we are normalising the data by diving it with 255 for this first conerting to float type
# by doing so,we can reduce the negative impact on the network
# now, the input features are scaled in between 0.0 to 1.0

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train = x_train/255.0
x_test = x_test/255.0

# Here, the image either belongs to one class or not and thus binary classification is done
# For this we are actually doing one-hot encoding with the help to-categorical

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
class_num = y_test.shape[1]

# Designing the model

# The first layer is the convolution layer and the size of the input is 32,32,3; 3 indicating color images
# The size of the filter used here is 3,3
# Most effectively used activation to predict non-negative values is relu
# Padding is same ie. size of the image remains the same
# Pooling layer is used to make robust image classifier and to learn relevant patterns

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))

# Dropout is to reduce overfitting and it removes 20% of  the connection between the layers

model.add(Dropout(0.2))

# Batch normalisation normalises the layer heaading to the next level 
# hence creating activations with same distributions

model.add(BatchNormalization())

# Here comes the convolution layer 
# The filter size is increased so the moel learns complex patterns

model.add(layers.Conv2D(64, (3, 3), padding = 'same'))
model.add(Activation('relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(BatchNormalization())

# Repeating the same by adding more filters........

model.add(layers.Conv2D(128, (3, 3), padding = 'same'))
model.add(Activation('relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(layers.Conv2D(128, (3, 3), padding = 'same'))
model.add(Activation('relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(BatchNormalization())

# Flattening the data

model.add(Flatten())

model.add(Dropout(0.2))

# We now make use of dense connected layer
# Number of neurons must be defined
# Number of neurons in the suceeding layers decreses eventually approaching the num of classes(10)
# Kernel constraint is used to regularize the data therby preventing overfitting

model.add(Dense(256, kernel_constraint = maxnorm(3)))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())

model.add(Dense(256, kernel_constraint = maxnorm(3)))
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(BatchNormalization())

# In the final layer passing no.of neurons as the no.of classes
# Each neuron represents a class and the output will be 10 neuron vector with each neuron containing 
# the probability of the image

# Finally, Softmax function selects the highest probability as output
# Voting that the image belongs to this class

model.add(Dense(class_num))
model.add(Activation('softmax'))

# To compile we need no.of epochs to be trained
# Optimizer will tune the weights in the network to minimise the loss value
# Most effectively used Adam optimizer is used

epochs = 25
optimizer = 'adam'

# Compiling the model with chosen parameters

model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy'])

# Printing the model summary

print(model.summary())

# setting random seed for reproducibility, initializing random weights and
# in turn same network trained on same data gives different values

seed = 21
np.random.seed(seed)
model.fit(x_train, y_train, validation_data = (x_test,y_test), epochs = epochs, batch_size = 64)

# Evaluating the model

score = model.evaluate(x_test, y_test, verbose = 0)
print('Accuracy: %.2f%%' % (score[1] * 100))

